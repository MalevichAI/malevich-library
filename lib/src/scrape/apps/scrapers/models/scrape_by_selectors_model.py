# generated by datamodel-codegen:
#   filename:  scrape_by_selectors_model.json

from __future__ import annotations
from malevich.square import scheme

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


@scheme()
class ScrapeBySelectors(BaseModel):
    allowed_domains: Optional[List[str]] = Field(
        None, description='A list of allowed domains to scrape'
    )
    components: Optional[List[Dict[str, Any]]] = Field(
        [], description='A list of components with keys and Xpaths need to be found'
    )
    output_type: Optional[str] = Field(
        'single_table', description='Output type of scraping results'
    )
    output_delimeter: Optional[str] = Field(
        '\n',
        description='If output_type is text, will use this delimeter to combine values',
    )
    include_keys: Optional[bool] = Field(
        False,
        description='If output_type is text and set to True, will include keys into the text',
    )
    max_depth: Optional[int] = Field(
        0, description='The maximum depth to traverse the web'
    )
    spider_cfg: Optional[Dict[str, Any]] = Field(
        {}, description='A dictionary of configuration options for the spider'
    )
    max_results: Optional[int] = Field(
        None, description='The maximum number of results to return'
    )
    timeout: Optional[int] = Field(
        0,
        description='The maximum number of seconds to wait for collecting responses from the spiders',
    )
    squash_results: Optional[bool] = Field(
        False,
        description='If set, the app will squash the results into a single string separated by the `squash_delimiter` option',
    )
    delimiter: Optional[str] = Field(
        "'\n'",
        description='The delimiter to use when squashing the results or when using independent crawl',
    )
    links_are_independent: Optional[bool] = Field(
        False, description='If set, the app will crawl each link independently'
    )
