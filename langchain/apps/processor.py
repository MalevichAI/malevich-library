# import concurrent.futures
# from itertools import product
# from typing import Any

# import pandas as pd
# from malevich.square import DF, DFS, Context, init, processor, scheme
# from pydantic import BaseModel

# from langchain.chat_models.base import BaseChatModel
# from langchain.schema import HumanMessage

# from .init import initialize_langchain as __initialize_langchain


# @scheme()
# class Prompt(BaseModel):
#     prompt: str


# @init(prepare=True)
# def initialize_langchain(ctx: Context):
#     """Initialize the langchain app.

#     Initializes two objects:
#         - Embedder (ctx.app_cfg["embedder"]) - used for embeddings
#         - Chat (ctx.app_cfg["chat"]) - used for chat

#     """
#     __initialize_langchain(ctx)




# @processor()
# def batch_prompted_messages(input_variables: DF[Any], ctx: Context):
#     """Send a single formatted message to the AI Chatbot in a batch.

#     Input:
#         Two required inputs to this processor are:
#             - input_variables: A dataframe with the variables to be used in the prompt template
#             - prompt_template: A string with the prompt template

#         While `input_variables` might came from the previous processor, `prompt_template` is
#         a constant string defined in the app configuration.

#     Configuration:
#         `prompt_template` - a string with the prompt template. The template might contain
#         variables in the following format: {variable_name}. The variables will be replaced
#         with the values from the `input_variables` dataframe for each row.

#     Details:
#         Each row in the `input_variables` dataframe will be used to generate a single message
#         to the AI Chatbot. The message will be generated by replacing the variables in the
#         `prompt_template` with the values from the row. Messages will be sent to the AI Chatbot
#         in a batch. The AI Chatbot will respond with a single message for each input message.
#         The context is not shared between the messages: each message is processed independently.

#     Output:
#         A dataframe with the following columns:
#             - result: AI Chatbot response to the input message

#     Examples:
#         Let's assume you want to come up with a name for your company services. You have
#         a list of words that you want to use in the name. You want to generate a list of
#         possible names by combining the words in different ways. You can use the following
#         configuration:

#         ```
#         prompt_template: "Derive a name for our company services. The name should contain the word {word1} and the word {word2}."
#         input_variables:
#         | word1 | word2 |
#         |-------|-------|
#         | power | energy|
#         | red   | panda |
#         ```

#         The processor will generate the following messages:

#             First message:
#             ```
#             Derive a name for our company services. The name should contain the word power and the word energy.
#             ```

#             Second message:
#             ```
#             Derive a name for our company services. The name should contain the word red and the word panda.
#             ```

#         Each message will be sent to the AI Chatbot. The AI Chatbot will respond with a single
#         message for each input message. The messages will be returned as a dataframe with a single
#         column `result`:

#         ```
#         | result |
#         |--------|
#         | Power Energy Services |
#         | Red Panda Services    |
#         ```


#     Args:
#         input_variables (DF[Any]): A dataframe with the variables to be used in the prompt template

#     Returns:
#         DF[str]: A dataframe with the following columns:
#             - result: AI Chatbot response to the input message
#     """  # noqa: E501
#     if 'prompt_template' not in ctx.app_cfg:
#         raise ValueError(
#             "The processor requires a `prompt_template` in the app configuration"
#         )


#     prompt_template = ctx.app_cfg.get("prompt_template")
#     return __df_prompt_format(input_variables, prompt_template, ctx)


# @processor()
# def batch_with_one_dynamic_prompt(
#     prompt: DF[Prompt],
#     input_variables: DF[Any] , context: Context):
#     """
#     This function is an extension of the 'process_langchain_request' processor above
#     as it can accept the prompt template as a dataframe collection of basically one cell.
#     Args:
#     input_variables: the prompt variables
#     prompt: the langchain prompt

#     Returns:

#     """
#     if len(prompt) > 1 or len(prompt.columns) > 1:
#         raise ValueError("the processor expects a prompt saved in a one-cell dataframe\n"
#                          f"Found: {len(prompt)} rows, {len(prompt.columns)} columns")

#     prompt_str = prompt.iloc[0, 0]

#     results = chat_one_message(input_variables, prompt_str, context)

#     return results


# # noinspection PyIncorrectDocstring
# @jls.processor()
# def embed_texts(docs: DF['text'], ctx: Context):
#     """Attach embeddings to the documents.

#     To use this processor, you have to pass the dataframe with the following columns:
#         - text (str) - text to embed

#     Example:
#         docs = pd.DataFrame({"text": ["This is a text", "This is another text"]})
#         docs_with_embeddings = embed_texts(docs)

#     Args:
#         docs (pd.DataFrame): Input dataframe

#     Returns:
#         pd.DataFrame: Output dataframe with the following columns:
#             - text (str) - text
#             - embeddings (list) - list of embeddings
#     """
#     embedder = ctx.app_cfg["embedder"]
#     docs["embeddings"] = embedder.embed_documents(docs.text.tolist())
#     return docs


# @jls.processor()
# def compare_texts(texts: DFS['text', 'text'], ctx: Context):
#     """Compares two group of texts
    
#     To tweak the behaviour of the processor, you might use the following parameters in the config:
#         - compare_method (str): 
#             Similarity assessment method. Possible values: cosine, euclidean
#         - compare_strategy (str):
#             Strategy for comparing texts. Possible values: rowwise, cartesian
#             If rowwise, the processor will compare texts row by row.
#             If cartesian, the processor will compare all texts from the first group with all texts from the second group.
    
#     Args:
#         texts (tuple): Tuple of two dataframes with the following columns:
#             - text (str) - text to embed
    
#     Returns:    
#         pd.DataFrame: Output dataframe with the following columns:
#             - text1 (str) - text from the first group
#             - text2 (str) - text from the second group
#             - score (float) - similarity score    
#     """
#     from langchain.embeddings.base import Embeddings
#     from numpy import dot
#     from numpy.linalg import norm

#     left_texts = texts[0]
#     right_texts = texts[1]
#     embedder: Embeddings = ctx.app_cfg["embedder"]
#     # ________________________________________

#     # Derive embeddings for both dataframes
#     left_embeddings = embedder.embed_documents(
#         left_texts.text.to_list()
#     )

#     right_embeddings = embedder.embed_documents(
#         right_texts.text.to_list()
#     )

#     compare_strategy = ctx.app_cfg.get('compare_strategy', 'rowwise')

#     if compare_strategy == 'rowwise':
#         _iter = zip(enumerate(left_embeddings), enumerate(right_embeddings))
#     elif compare_strategy == 'product':
#         _iter = product(enumerate(left_embeddings), enumerate(right_embeddings))

#     outputs = {
#         "text1": [],
#         "text2": [],
#         "score": [],
#     }

#     for (ia, a), (ib, b) in _iter:
#         if ctx.app_cfg.get("compare_method", "cosine") == "cosine":
#             similarity = dot(a, b) / (norm(a) * norm(b))
#         elif ctx.app_cfg.get("compare_method") == "euclidean":
#             similarity = norm(a - b)

#         outputs["text1"].append(left_texts.text.iloc[ia])
#         outputs["text2"].append(right_texts.text.iloc[ib])
#         outputs["score"].append(similarity)

#     return pd.DataFrame(outputs)
